\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Investigating the Effect of Dataset Representivity for Image Colourisation}

\author{\IEEEauthorblockN{Andrew Boyley}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{and Applied Mathematics}\\
\textit{University of the Witwatersrand}\\
Johannesburg, South Africa \\
andrew.boyley@students.wits.ac.za}
\and
\IEEEauthorblockN{William Hill} 
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{and Applied Mathematics}\\
\textit{University of the Witwatersrand}\\
Johannesburg, South Africa \\
william.hill1@students.wits.ac.za}
\and
\IEEEauthorblockN{Steven James}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{and Applied Mathematics}\\
\textit{University of the Witwatersrand}\\
Johannesburg, South Africa \\
steven.james@wits.ac.za}
}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
deep learning, bias, dataset imbalance, regression, self-supervised learning
\end{IEEEkeywords}

\section{Introduction}

Well known that bias in ML is a problem. 

Due to lack of diversity in training examples

Some work on how to mitigate these issues in training, but what about the data itself?

We investigate how much data we need to make the problem go away. 

Expensive to construct datasets. Need time, money, info.
We therefore select the task of image colourisation, since it is a supervised approach. We need no labelling, only data, since the regression targets are derived from the images themselves!
We get this data from publically accessible sources captured from variety of places across SA/Africa. 
Easy to build a diverse dataset in this way. 

We train image colourisation on dataset X, and test on our dataset Y.  I assume bad things happpen.

We look at how much data we need to augment X from Y to fix the problem. We find (insert results) 

\section{Background}

%Explain notation and setup for regression/classification task in general i.e. objective function

%Describe LAB; classes, etc. 

%Describe eval criteria

%Describe neural network, backprop, Insert image if space
\subsection{Colour space}

To make our lives easier, instead of using the typical RGB color space, we use the CIELAB color space. This makes it convenient when seperating the gray and color channels due to the fact that the 'L' channel in LAB is simply the grayscale image. We can then pass this grayscale input (1xWxH) through our model and compare the output (2xWxH) to the original 'AB' channels of the image. These 'AB' channels make up the color and simply show the red-green and blue-yellow shift respectively. HSB would be another option when choosing a suitable colour space. In general, the choice of colour space makes no difference to the resulting image that our network produces.

When reading images from our dataset, we first interpret them in the RBG colour space and then proceed to convert them to LAB using tools such as SciKit Learn or OpenCV, both of which contain colour space conversion libraries. 

\subsection{Regression vs Classification}

When colorizing images using a deep learning approach, it is generally easiest to develop our model as a regression problem. Classification would yield better results which are more life-like and saturated whereas regression would lead to output images appearing dull and bland. This is the result of the multi-modality of pictures. There is no clear color for any one image. This leads our model, using an aggregate loss function, to simply find an average color, resulting in a bland image.

For the sake of simplicity, we continue to use regression as it leads to satisfactory results whilst being simple to understand. We utilize a mean squared error loss which may average color distribution but still produces images with a reasonable amount of color. If we aimed to produce images which were more diverse in colour and saturated, regression would not suffice. Our model should, however, be able to compare output across datasets in order to draw correlation between the accuracy of the ouput and the representivity of local images in the data, which it does.

\subsection{Model topology}

Our choice of model has been based on the simple regression model developed by Luke Melas and loosely based on the network used by Zhang et al. with the major difference being that their model used in 'Colorful Image Colorization' approached colorization as a classification problem.

Our network topology consists of the first six layers of a ResNet-18, followed by three layers of deconvolutions. We find that this simple structure works well enough for our purposes and is relatively fast. Our model uses the Adam optimiser alongside PyTorch's autograd which makes backpropogating in our network trivial.

\section{Proposed Method}

Train colouriser on dataset X. Evaluate accuracy

Explain how our dataset is built precisely: using flickr data, keywords, any preprocessing etc. 

Show some images. 

Maybe even compute mean statistics of dataset X and Y to show that they are in fact different?

Explain method - compute accuracy on X -> X.
Then check X -> Y

Then check how much of Y must be added to X to improve things?

Would be nice to have multiple datasets, models, but maybe no time :(

\section{Experiments}

Explain experiment. Model used. Hyperparameters. Early stopping?? Train/val/test splits.

Show training curves over time for model trained on X. 

Show images from X coloured.
Show images from Y coloured - qualitative analysis

Start adding data from Y to X. 

Produce THE curve as a function of data ratio

Show images from X coloured.
Show images from Y coloured - qualitative analysis



\section{Related Work}

\begin{itemize}
    \item Find literature on dataset imbalance in general
    \item Find literature on racial bias in general
\end{itemize}

\section{Conclusion}

Important problem

Investigated data representivity

What we found

Extensions to other tasks.


\bibliographystyle{IEEEtran}
\bibliography{IEEEfull,colourisation}

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}

\end{document}
