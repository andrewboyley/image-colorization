{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "from skimage import io,transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diaply and save images\n",
    "def visualize_image(grayscale_input, ab_input=None, show_image=False, save_path=None, save_name=None):\n",
    "        '''Show or save image given grayscale (and ab color) inputs. Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "        plt.clf() # clear matplotlib plot\n",
    "        ab_input = ab_input.cpu()\n",
    "        grayscale_input = grayscale_input.cpu()    \n",
    "        if ab_input is None:\n",
    "            # save and display grayscale image only\n",
    "            grayscale_input = grayscale_input.squeeze().numpy() \n",
    "            if save_path is not None and save_name is not None: \n",
    "                plt.imsave(grayscale_input, '{}.{}'.format(save_path['grayscale'], save_name) , cmap='gray')\n",
    "            if show_image: \n",
    "                plt.imshow(grayscale_input, cmap='gray')\n",
    "                plt.show()\n",
    "        else: \n",
    "            # save grayscale and RGB image\n",
    "            # display RGB image\n",
    "            color_image = torch.cat((grayscale_input, ab_input), 0).numpy() #combines the l and ab dimensions to one dimension\n",
    "            color_image = color_image.transpose((1, 2, 0))  \n",
    "            color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "            color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
    "            color_image = lab2rgb(color_image.astype(np.float64)) #convert coloured LAB image to RGB to display\n",
    "            grayscale_input = grayscale_input.squeeze().numpy()\n",
    "            if save_path is not None and save_name is not None:\n",
    "                plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "                plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n",
    "            if show_image: \n",
    "                plt.imshow(color_image)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeopleDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self,root_directory,transform=None):\n",
    "        self.file_names = os.listdir(root_directory)\n",
    "        self.root_dir = root_directory\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        image = io.imread(self.root_dir + self.file_names[index])\n",
    "            \n",
    "        image_lab = self.get_lab(image)\n",
    "        image_ab = self.get_ab(image_lab)\n",
    "        image_l = self.get_l(image_lab)\n",
    "        \n",
    "        return image_l,image_ab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "        \n",
    "    def get_ab(self,lab_image):\n",
    "        img_ab = lab_image[:, :, 1:3]\n",
    "        img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "        return img_ab\n",
    "        \n",
    "    def get_l(self,image):\n",
    "        img_l = rgb2gray(image)\n",
    "        img_l = torch.from_numpy(img_l).unsqueeze(0).float()\n",
    "        return img_l\n",
    "        \n",
    "    def get_lab(self,rgb_image):\n",
    "        rgb_image = np.asarray(rgb_image)\n",
    "        img_lab = rgb2lab(rgb_image)\n",
    "        img_lab = (img_lab + 128) / 255\n",
    "        \n",
    "        img_lab_resized = transform.resize(img_lab,(512,512))\n",
    "        return img_lab_resized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PeopleDataSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b85b931557df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeopleDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/andrew/Pictures/Datasets/INRIAPerson/Train/pos/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PeopleDataSet' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = PeopleDataSet(root_directory='/home/andrew/Pictures/Datasets/INRIAPerson/Train/pos/')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self, midlevel_input_size=128, global_input_size=512):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        # Fusion layer to combine midlevel and global features\n",
    "        self.midlevel_input_size = midlevel_input_size\n",
    "        self.global_input_size = global_input_size\n",
    "        self.fusion = nn.Linear(midlevel_input_size + global_input_size, midlevel_input_size)\n",
    "        self.bn1 = nn.BatchNorm1d(midlevel_input_size)\n",
    "\n",
    "        # Convolutional layers and upsampling\n",
    "        self.deconv1_new = nn.ConvTranspose2d(midlevel_input_size, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.Conv2d(midlevel_input_size, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "#         self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        print('Loaded colorization net.')\n",
    "\n",
    "    def forward(self, midlevel_input):\n",
    "        \n",
    "        x = self.conv1(midlevel_input)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        \n",
    "        # Build ResNet and change first conv layer to accept single-channel input\n",
    "        resnet_gray_model = models.resnet18(num_classes=365)\n",
    "        resnet_gray_model.conv1.weight = nn.Parameter(resnet_gray_model.conv1.weight.sum(dim=1).unsqueeze(1).data)\n",
    "\n",
    "        # Extract midlevel and global features from ResNet-gray\n",
    "        self.midlevel_resnet = nn.Sequential(*list(resnet_gray_model.children())[0:6])\n",
    "        self.fusion_and_colorization_net = ColorizationNet()\n",
    "\n",
    "    def forward(self, input_image):\n",
    "\n",
    "        # Pass input through ResNet-gray to extract features\n",
    "        midlevel_output = self.midlevel_resnet(input_image)\n",
    "\n",
    "        # Combine features in fusion layer and upsample\n",
    "        output = self.fusion_and_colorization_net(midlevel_output) #, global_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded colorization net.\n"
     ]
    }
   ],
   "source": [
    "net = ColorNet().to('cuda') if use_gpu else ColorNet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cd7b94593a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e1daee15b148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_ab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mimage_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_ab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_ab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for image_l,image_ab in train_loader:\n",
    "        if use_gpu:\n",
    "            image_l,image_ab = image_l.to('cuda'),image_ab.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(image_l)\n",
    "        \n",
    "        loss = criterion(output, image_ab)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch',epoch,'Loss',total_loss.item())\n",
    "    \n",
    "    # See image\n",
    "    visualize_image(image_l, output, show_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
