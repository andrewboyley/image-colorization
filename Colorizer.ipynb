{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "from skimage import io,transform\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeopleDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self,root_directory,transform=None):\n",
    "        self.file_names = os.listdir(root_directory)\n",
    "        self.root_dir = root_directory\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        image = io.imread(self.root_dir + self.file_names[index])\n",
    "            \n",
    "        image_lab = self.get_lab(image)\n",
    "        image_ab = self.get_ab(image_lab)\n",
    "        image_l = self.get_l(image_lab)\n",
    "        \n",
    "        return image_l,image_ab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "        \n",
    "    def get_ab(self,lab_image):\n",
    "        img_ab = lab_image[:, :, 1:3]\n",
    "        img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "        return img_ab\n",
    "        \n",
    "    def get_l(self,image):\n",
    "        img_l = rgb2gray(image)\n",
    "        img_l = torch.from_numpy(img_l).unsqueeze(0).float()\n",
    "        return img_l\n",
    "        \n",
    "    def get_lab(self,rgb_image):\n",
    "        rgb_image = np.asarray(rgb_image)\n",
    "        img_lab = rgb2lab(rgb_image)\n",
    "        img_lab = (img_lab + 128) / 255\n",
    "        \n",
    "        img_lab_resized = transform.resize(img_lab,(512,512))\n",
    "        return img_lab_resized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PeopleDataSet(root_directory='/home/andrew/Pictures/Datasets/INRIAPerson/Train/pos/')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self, midlevel_input_size=128, global_input_size=512):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        # Fusion layer to combine midlevel and global features\n",
    "        self.midlevel_input_size = midlevel_input_size\n",
    "        self.global_input_size = global_input_size\n",
    "        self.fusion = nn.Linear(midlevel_input_size + global_input_size, midlevel_input_size)\n",
    "        self.bn1 = nn.BatchNorm1d(midlevel_input_size)\n",
    "\n",
    "        # Convolutional layers and upsampling\n",
    "        self.deconv1_new = nn.ConvTranspose2d(midlevel_input_size, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.Conv2d(midlevel_input_size, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "#         self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        print('Loaded colorization net.')\n",
    "\n",
    "    def forward(self, midlevel_input):\n",
    "        \n",
    "        x = self.conv1(midlevel_input)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        \n",
    "        # Build ResNet and change first conv layer to accept single-channel input\n",
    "        resnet_gray_model = models.resnet18(num_classes=365)\n",
    "        resnet_gray_model.conv1.weight = nn.Parameter(resnet_gray_model.conv1.weight.sum(dim=1).unsqueeze(1).data)\n",
    "\n",
    "        # Extract midlevel and global features from ResNet-gray\n",
    "        self.midlevel_resnet = nn.Sequential(*list(resnet_gray_model.children())[0:6])\n",
    "        self.fusion_and_colorization_net = ColorizationNet()\n",
    "\n",
    "    def forward(self, input_image):\n",
    "\n",
    "        # Pass input through ResNet-gray to extract features\n",
    "        midlevel_output = self.midlevel_resnet(input_image)\n",
    "\n",
    "        # Combine features in fusion layer and upsample\n",
    "        output = self.fusion_and_colorization_net(midlevel_output) #, global_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded colorization net.\n"
     ]
    }
   ],
   "source": [
    "net = ColorNet().to('cuda') if use_gpu else ColorNet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 4.646118640899658\n",
      "Epoch 1 Loss 0.1637272983789444\n",
      "Epoch 2 Loss 0.07120449841022491\n",
      "Epoch 3 Loss 0.06606699526309967\n",
      "Epoch 4 Loss 0.06375345587730408\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for image_l,image_ab in train_loader:\n",
    "        if use_gpu:\n",
    "            image_l,image_ab = image_l.to('cuda'),image_ab.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(image_l)\n",
    "        \n",
    "        loss = criterion(output, image_ab)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch',epoch,'Loss',total_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
